The relevant data can likely be mmapped simply by passing a buffer object alongside existing functions and reusing it where desired.

- visit __setstate__ etc and construct data as directly backed by bytes or memoryview
- use a buffer format that pickle preserves the bytes object from as read, this may be already the case or may require code mutation
- unpickle file object that returns mmap'd data from read()

could also use a different storage format if working on system with enough ram to convert

---

torch uses some custom unpickling logic already
in torch/serialization.py.

the load_tensor() local function is on line 841 for me.

        it looks like the data might be zip-packed, and unpacked when loaded/referenced, unsure
        i'm on line 846, with storage loaded from the zip file, in the debugger, and i typed `p storage` to see what it was
        memory allocation is rising by the gigabyte as it waits to output the value, as if it is loading the entire file into ram
        when i typed 's' on line 845 where zip_file.get_storage_from_record was called, it did not step into it, so it could be compiled code

        on this line, thrashing is not yet too bad, despite vram usage rising.  thrashing is rising a little bit, though.

            the zipfile is made via _open_zipfile_reader which passes the name or buffer through torch._C.PyTorchFileReader

        it's looking like it might be simpler to make one's own loading format.
        parameters are just a dictionary of tensors, as i understand.  yeah, an ordered dictionary.
            torch.frombuffer(buffer, dtype=dtype, count=num_items, offset=bytes_to_skip, requires_grad=TrueOrFalse) <- produces a tensor view of something that implements the buffer protocol
               

	note: mmap offsets must be page aligned, which usually means 4k
